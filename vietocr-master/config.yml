vocab: 3cghad6bfrp72ewxm5k84yn
device: cpu
seq_modeling: transformer
transformer:
  d_model: 256
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 2048
  max_seq_length: 1024
  pos_dropout: 0.1
  trans_dropout: 0.1
optimizer:
  max_lr: 0.0003
  pct_start: 0.1
trainer:
  batch_size: 32
  print_every: 200
  valid_every: 4000
  iters: 100000
  export: ./weights/transformerocr.pth
  checkpoint: ./checkpoint/transformerocr_checkpoint.pth
  log: ./train.log
  metrics: None
dataset:
  name: data
  data_root: ./img/
  train_annotation: annotation_train.txt
  valid_annotation: annotation_val_small.txt
  image_height: 32
  image_min_width: 32
  image_max_width: 512
dataloader:
  num_workers: 3
  pin_memory: 'True'
aug:
  image_aug: 'True'
  masked_language_model: 'True'
predictor:
  beamsearch: 'False'
quiet: 'False'
pretrain: https://vocr.vn/data/vietocr/vgg_transformer.pth
weights: /data/huydq46/RPA_Captcha_OCR/transformerocr.pth
backbone: vgg19_bn
cnn:
  pretrained: 'True'
  ss:
    - - 2
      - 2
    - - 2
      - 2
    - - 2
      - 1
    - - 2
      - 1
    - - 1
      - 1
  ks:
    - - 2
      - 2
    - - 2
      - 2
    - - 2
      - 1
    - - 2
      - 1
    - - 1
      - 1
  hidden: 256
